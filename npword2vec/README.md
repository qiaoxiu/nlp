这个是他人实现，  实现注释 自己的理解   更好的理解word2vec词向量地址为： https://github.com/multiangle/pyword2vec.git


word2vec 其宗旨是使用分布式向量来表示句子，在以前表示句子的方式有tf-idf one-hote编码等，存在稀疏 不能对句子之间进行计算等问题，google提出了句子表示 word embedding
相似的词距离更近 否者相反，根据语料的无监督学习方式来压缩矩阵达到1-300维向量来表示一个词 并且词于词之间有语义方面的信息

word2vec 使用神经网络实现词向量的学习，根据输入 输出方式分为cbow skipgram 根据优化方式 层次softmax 和 负采样

cbox 上下文词预测中心词   输入是上下文词向量的加和 输出 是 词汇表大小的向量得分
skipgram 中心词预测上下文词  输入是中心词 输出是 2n个 词汇表大小的词向量得分 

由于 针对词向量大小的矩阵进行softmax 以及反向传播 训练起来难以工作，逐采用优化方案来学习词向量，所以采用 层次softmax  和 负采样来实现 学习优化

层次softmax 根据语料进行切分 词频统计 构建 霍夫曼树 ，根据霍夫曼树 获得霍夫曼编码，因为词频大的排在树的前面，从而使加权路径最小的特点，只需要很少几部二分类就可以定位到
目标label词，然后在计算没经过的路径上的loss 所以能大幅提高学习效率

学习过程有俩个参数进行学习  x 输入词向量 在叶节点 xite 树的非叶节点 也叫中间向量 
根据语言模型的定义和优化目标函数最终的公式是
xite = xite + grad*(1-dw-q(wx))xite 
x = x + grad*(1-dw-q(wx))x 

q 是二分类激活函数 

负采样是另一种训练优化方案 比层次softmax效率高
